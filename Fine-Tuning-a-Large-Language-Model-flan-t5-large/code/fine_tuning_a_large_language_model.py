# -*- coding: utf-8 -*-
"""Fine-Tuning a Large Language Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bZJI2AHdjKj6dGe04mWM-xZ1Oma8Fodr
"""

!pip install transformers datasets accelerate peft bitsandbytes --upgrade

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

model_name = "google/flan-t5-large"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForSeq2SeqLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q", "v"],
    lora_dropout=0.05,
    bias="none",
    task_type="SEQ_2_SEQ_LM"
)

model = get_peft_model(model, lora_config)

from google.colab import files
uploaded = files.upload()

def preprocess(batch):
    input_texts = ["Java Question: " + q for q in batch["instruction"]]
    target_texts = batch["output"]
    return tokenizer(
        input_texts,
        text_target=target_texts,
        truncation=True,
        padding="max_length",
        max_length=256,
    )

from datasets import load_dataset

# Load dataset
dataset = load_dataset("json", data_files="java_qa_dataset.json")["train"]

# Split dataset: 80% train, 10% eval, 10% test
train_test = dataset.train_test_split(test_size=0.2, seed=42)
eval_test = train_test["test"].train_test_split(test_size=0.5, seed=42)

train_data = train_test["train"]
eval_data = eval_test["train"]
test_data = eval_test["test"]

tokenized_train = train_data.map(preprocess, batched=True)
tokenized_eval = eval_data.map(preprocess, batched=True)
tokenized_test = test_data.map(preprocess, batched=True)

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./flan-t5-lora-java-results",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    learning_rate=2e-4,
    num_train_epochs=5,
    logging_dir="./logs",
    logging_steps=10,
    save_strategy="epoch",
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    tokenizer=tokenizer
)

trainer.train()
model.save_pretrained("./flan-t5-lora-java-results")

from transformers import AutoTokenizer
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
model.eval()

java_questions = [
    "What is the difference between final, finally and finalize in Java?",
    "What is method overloading in Java?",
    "What is garbage collection?",
    "Explain static vs instance variables.",
    "What is the use of final keyword?"
]

for question in java_questions:
    input_text = "Java Question: " + question
    inputs = tokenizer(input_text, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=False,
            temperature=0.7,
            repetition_penalty=1.2
        )

    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\nüß† Q: {question}\n‚úÖ A: {answer}")

!pip install evaluate rouge_score

import evaluate

bleu = evaluate.load("bleu")
rouge = evaluate.load("rouge")

# Sample questions (can use test_data here if needed)
sample_questions = [
    "What is a constructor in Java?",
    "What is method overloading in Java?",
    "What is garbage collection?",
]

reference_answers = [
    "A constructor is a special method used to initialize objects in Java.",
    "Method overloading is when multiple methods have the same name but different parameters.",
    "Garbage collection is the process of automatically reclaiming memory used by unreferenced objects."
]

predicted_answers = []
for q in sample_questions:
    inputs = tokenizer("Java Question: " + q, return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=100)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    predicted_answers.append(answer)

# Evaluate
bleu_score = bleu.compute(predictions=predicted_answers, references=[[r] for r in reference_answers])
rouge_score = rouge.compute(predictions=predicted_answers, references=reference_answers)

print("\nüìä Evaluation Metrics:")
print(f"BLEU Score     : {bleu_score['bleu']:.4f}")
print(f"ROUGE-1        : {rouge_score['rouge1']:.4f}")
print(f"ROUGE-2        : {rouge_score['rouge2']:.4f}")
print(f"ROUGE-L        : {rouge_score['rougeL']:.4f}")

!pip install gradio --upgrade

import gradio as gr
import torch


def answer_java_question(question):
    prompt = "Answer this Java programming question clearly:\nJava Question: " + question
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=128,
            do_sample=True,
            temperature=0.8,
            top_k=50,
            top_p=0.95,
            repetition_penalty=1.2
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# --- Theme Customization ---
theme = gr.themes.Default(
    primary_hue="indigo",
    secondary_hue="blue"
).set(
    body_background_fill="#f8fafc",
    container_radius="xl",
    input_background_fill="white",
    input_border_color="#cbd5e1",
    input_border_width="1px",
    button_primary_background_fill="indigo",
    block_title_text_weight="bold",
    block_title_text_size="xl"
)

# --- Interface Layout ---
with gr.Blocks(theme=theme, title="Java Programming Q&A Bot", css="""
    #main-container { max-width: 800px; margin: auto; }
    #footer {
        margin-top: 2em;
        color: #94a3b8;
        font-style: italic;
        text-align: center;
        font-size: 0.9em;
    }
""") as demo:
    gr.Image("https://upload.wikimedia.org/wikipedia/en/3/30/Java_programming_language_logo.svg", width=80)
    gr.Markdown("## üí° Java Programming Q&A Bot")
    gr.Markdown("üîç Ask any **Java-related** programming question and get accurate, AI-powered answers instantly!")

    with gr.Row():
        with gr.Column(scale=3):
            input_box = gr.Textbox(
                lines=3,
                placeholder="Enter your Java programming question here...",
                label="‚ùì Your Java Question"
            )
            submit_btn = gr.Button("Get Answer üí¨")

        with gr.Column(scale=2):
            output_box = gr.Textbox(
                label="‚úÖ AI Response",
                lines=6,
                interactive=False
            )

    # Example questions
    examples = gr.Examples(
        examples=[
            "What is a constructor in Java?",
            "What is the difference between abstract class and interface?",
            "How does garbage collection work in Java?",
            "What is the use of 'final' keyword in Java?",
            "Explain static vs instance variables in Java."
        ],
        inputs=[input_box]
    )

    # Footer
    gr.Markdown(
        "üí¨ Powered by `flan-t5-large` fine-tuned with LoRA ¬∑ Built with ‚ù§Ô∏è by Thivya Dhanasegaran",
        elem_id="footer"
    )

    # Connect input/output
    submit_btn.click(fn=answer_java_question, inputs=input_box, outputs=output_box, show_progress="full")

# --- Launch App ---
demo.launch(share=True)